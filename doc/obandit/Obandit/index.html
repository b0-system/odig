<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Obandit (obandit.Obandit)</title><link rel="stylesheet" href="../../_odoc-theme/odoc.css"/><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div class="content"><header><nav><a href="../index.html">Up</a> – <a href="../index.html">obandit</a> &#x00BB; Obandit</nav><h1>Module <code>Obandit</code></h1><p>Ocaml Multi-Armed Bandits</p><p>Obandit is an Ocaml module for multi-armed bandits. It supports the EXP, UCB and Epsilon-greedy family of algorithms.</p><p><em>Version v0.3.4 - <a href="http://freux.fr/oss/obandit.html">homepage</a></em></p> <img src="http://freux.fr/kroliki.jpg" alt="rabbits" width="600"> <h2 id="mutband"><a href="#mutband" class="anchor"></a>Bandit Modules</h2><p>This library implements multi-armed bandits as modules. A bandit module is obtained by calling a functor with a bandit module parameter. The parameter usually gives the number $K$ of arms and the hyperparameters of the bandit.</p><nav class="toc"><ul><li><a href="#mdf">The UCB family of algorithms.</a><ul><li><a href="#apu">MakeAlphaPhiUCB: $(\alpha,\psi)$-UCB</a></li><li><a href="#apu">MakeAlphaUCB: $\alpha$-UCB</a></li><li><a href="#apu">MakeUCB1: UCB1</a></li></ul></li><li><a href="#mdf">The Epsilon-Greedy family of algorithms.</a><ul><li><a href="#mdf">MakeParametrizableEpsilonGreedy: $\epsilon$-Greedy with a parametrizable rate.</a></li><li><a href="#mdf">MakeDecayingEpsilonGreedy: $ \epsilon_n$-Greedy with the decaying rate from <code>[5]</code>.</a></li><li><a href="#mdf">MakeEpsilonGreedy: $ \epsilon_n$-Greedy with a fixed exploration rate.</a></li></ul></li><li><a href="#mdf">The Exp3 family of algorithms.</a><ul><li><a href="#mdf">MakeExp3: EXP3 with a parametrizable rate.</a></li><li><a href="#mdf">MakeDecayingExp3: EXP3 with the decaying rate from <code>[1]</code>.</a></li><li><a href="#mdf">MakeFixedExp3: EXP3 with a fixed rate.</a></li><li><a href="#mdf">MakeHorizonExp3: EXP3 with a known horizon <code>[1]</code>.</a></li></ul></li><li><a href="#mdf">More Functors: The doubling trick.</a></li><li><a href="#ex">Examples</a></li><li><a href="#refs">References</a></li></ul></nav></header><dl><dt class="spec module-type" id="module-type-Bandit"><a href="#module-type-Bandit" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-Bandit/index.html">Bandit</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>A bandit algorithm.</p></dd></dl><aside><p>$$ $$</p><p>Bandit modules are instanciated using functors. Depending on the algorithm type used, the module parameter varies.</p><p>For instance, the UCB1 bandit module for 3 arms is obtained with:</p><p><code>module UCB1 =
  let module P = struct
  let k=3
  end
  in MakeUCB1(P)</code></p><p>The following algorithms are available:</p></aside><section><header><h3 id="mdf"><a href="#mdf" class="anchor"></a>The UCB family of algorithms.</h3><p>We use the viewpoint of the survey <code>[1]</code>.</p></header><section><header><h4 id="apu"><a href="#apu" class="anchor"></a>MakeAlphaPhiUCB: $(\alpha,\psi)$-UCB</h4><p>At time $t$, the $(\alpha,\psi)$-UCB algorithm<code>[1]</code> is taking action:</p><p>$$ \argmax_{i=1,\cdots,K} \quad \left[ \widehat{\mu_i}+(\psi^{*})^{-1}\left(\frac{\alpha\ln t}{T_i} \right) \right] $$</p><p>where $\alpha &gt; 0$ is a hyperparameter, $\widehat{\mu_i}$ is the estimate of the average reward of arm $i$, $T_i$ is the number of times arm $ i$ was visited so far and $\psi^*$ denotes the Legendre-Fenchel transform of a convex function $\psi$.</p><p>The pseudo-regret $\bar{R_n}$ has the following bound at round $n$: $$ \bar{R_n} \leq \sum_{i:\Delta_i &gt; 0} \left( \frac{\alpha \Delta_i}{\psi^* (\Delta_i / 2 )} \ln n + \frac{\alpha }{\alpha-2 } \right) $$</p><p>where $ \Delta_i = \mu^* - \mu_i $ is the suboptimality parameter of arm $ i$.</p></header><dl><dt class="spec type" id="type-banditEstimates"><a href="#type-banditEstimates" class="anchor"></a><code><span class="keyword">type</span> banditEstimates</code><code> = </code><code>{</code><table class="record"><tr id="type-banditEstimates.t" class="anchored"><td class="def field"><a href="#type-banditEstimates.t" class="anchor"></a><code>t : int;</code></td><td class="doc"><p>The index of the step</p></td></tr><tr id="type-banditEstimates.a" class="anchored"><td class="def field"><a href="#type-banditEstimates.a" class="anchor"></a><code>a : int;</code></td><td class="doc"><p>The last action taken.</p></td></tr><tr id="type-banditEstimates.nVisits" class="anchored"><td class="def field"><a href="#type-banditEstimates.nVisits" class="anchor"></a><code>nVisits : int list;</code></td><td class="doc"><p>The visit counts by arm.</p></td></tr><tr id="type-banditEstimates.u" class="anchored"><td class="def field"><a href="#type-banditEstimates.u" class="anchor"></a><code>u : float list;</code></td><td class="doc"><p>The cumulative arm reward observations.</p></td></tr></table><code>}</code></dt><dd><p>The inner state of a bandit that maintains estimates of arm means.</p></dd></dl><dl><dt class="spec module-type" id="module-type-AlphaPhiUCBParam"><a href="#module-type-AlphaPhiUCBParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-AlphaPhiUCBParam/index.html">AlphaPhiUCBParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeAlphaPhiUCB</code> by giving $\alpha$ and $\phi$.</p></dd></dl><dl><dt class="spec module" id="module-MakeAlphaPhiUCB"><a href="#module-MakeAlphaPhiUCB" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeAlphaPhiUCB/index.html">MakeAlphaPhiUCB</a> : <span class="keyword">functor</span> (<a href="MakeAlphaPhiUCB/argument-1-P/index.html">P</a> : <a href="index.html#module-type-AlphaPhiUCBParam">AlphaPhiUCBParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeAlphaPhiUCB/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The $(\alpha,\psi)$-UCB Bandit for stochastic regret minimization described in <code>[1]</code>.</p></dd></dl></section><section><header><h4 id="apu"><a href="#apu" class="anchor"></a>MakeAlphaUCB: $\alpha$-UCB</h4><p>The $\alpha$-UCB algorithm<code>[5]</code> uses $ \psi(\lambda) = \lambda^2 / 8 $. It chooses the action:</p><p>$$ \argmax_{i=1,\cdots,K} \left[ \widehat{\mu_i} + \sqrt{ \frac{\alpha \ln t}{2 T_i} } \right] $$</p><p>This gives a pseudo-regret bound of:</p><p>$$ \bar{ R_n} \leq \sum_{i:\Delta_i &gt; 0} \left( \frac{2 \alpha} { \Delta_i } \ln n + \frac{\alpha}{\alpha - 2} \right) $$</p></header><dl><dt class="spec module-type" id="module-type-AlphaUCBParam"><a href="#module-type-AlphaUCBParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-AlphaUCBParam/index.html">AlphaUCBParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeAlphaUCB</code> by giving $\alpha$.</p></dd></dl><dl><dt class="spec module" id="module-MakeAlphaUCB"><a href="#module-MakeAlphaUCB" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeAlphaUCB/index.html">MakeAlphaUCB</a> : <span class="keyword">functor</span> (<a href="MakeAlphaUCB/argument-1-P/index.html">P</a> : <a href="index.html#module-type-AlphaUCBParam">AlphaUCBParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeAlphaUCB/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The $\alpha$-UCB Bandit for stochastic regret minimization described in <code>[1]</code> .</p></dd></dl></section><section><header><h4 id="apu"><a href="#apu" class="anchor"></a>MakeUCB1: UCB1</h4><p>The UCB1 algorithm<code>[5]</code> uses $\alpha = 4$. It chooses the action:</p><p>$$ \argmax_{i=1,\cdots,K} \left[ \widehat{\mu_i} + \sqrt{ \frac{2 \ln t}{T_i} } \right] $$</p><p>At round $ n$, this gives a pseudo-regret bound of:</p><p>$$ \bar{R_n} \leq \sum_{i:\Delta_i &gt; 0} \left( \frac{8}{\Delta_i} \ln n + 2 \right) $$</p></header><dl><dt class="spec module-type" id="module-type-KBanditParam"><a href="#module-type-KBanditParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-KBanditParam/index.html">KBanditParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeUCB1</code>.</p></dd></dl><dl><dt class="spec module" id="module-MakeUCB1"><a href="#module-MakeUCB1" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeUCB1/index.html">MakeUCB1</a> : <span class="keyword">functor</span> (<a href="MakeUCB1/argument-1-P/index.html">P</a> : <a href="index.html#module-type-KBanditParam">KBanditParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeUCB1/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The UCB1 Bandit for stochastic regret minimization .</p></dd></dl></section></section><section><header><h3 id="mdf"><a href="#mdf" class="anchor"></a>The Epsilon-Greedy family of algorithms.</h3></header><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeParametrizableEpsilonGreedy: $\epsilon$-Greedy with a parametrizable rate.</h4><p>At round $t$, the $ \epsilon_t$-Greedy algorithm<code>[5]</code> takes action $\argmax_{i=1,\cdots,K} \widehat{\mu_i} $ with probability $ 1-\epsilon_t$ and an uniformly random action with probability $ \epsilon_t$.</p></header><dl><dt class="spec module-type" id="module-type-RateBanditParam"><a href="#module-type-RateBanditParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-RateBanditParam/index.html">RateBanditParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate algorithms that need a parametrizable rate.</p></dd></dl><dl><dt class="spec module" id="module-MakeParametrizableEpsilonGreedy"><a href="#module-MakeParametrizableEpsilonGreedy" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeParametrizableEpsilonGreedy/index.html">MakeParametrizableEpsilonGreedy</a> : <span class="keyword">functor</span> (<a href="MakeParametrizableEpsilonGreedy/argument-1-P/index.html">P</a> : <a href="index.html#module-type-RateBanditParam">RateBanditParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeParametrizableEpsilonGreedy/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The $\epsilon$-Greedy Bandit with a parametrizable exploration rate.</p></dd></dl></section><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeDecayingEpsilonGreedy: $ \epsilon_n$-Greedy with the decaying rate from <code>[5]</code>.</h4><p>This uses the exploration rate decay: $$ \epsilon_t = \min \left\{ 1, \frac{cK}{d^2 t} \right\} $$ where $ d &gt; 0 $ should be taken as a tight lower bound on $ \max_{i=1,\cdots,K} \Delta_i$ and $ c &gt; 0$ is a hyperparameter.</p></header><dl><dt class="spec module-type" id="module-type-DecayingEpsilonGreedyParam"><a href="#module-type-DecayingEpsilonGreedyParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-DecayingEpsilonGreedyParam/index.html">DecayingEpsilonGreedyParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeDecayingEpsilonGreedy</code> .</p></dd></dl><dl><dt class="spec module" id="module-MakeDecayingEpsilonGreedy"><a href="#module-MakeDecayingEpsilonGreedy" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeDecayingEpsilonGreedy/index.html">MakeDecayingEpsilonGreedy</a> : <span class="keyword">functor</span> (<a href="MakeDecayingEpsilonGreedy/argument-1-P/index.html">P</a> : <a href="index.html#module-type-DecayingEpsilonGreedyParam">DecayingEpsilonGreedyParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeDecayingEpsilonGreedy/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The Epsilon-Greedy Bandit with the decaying exploration rate from <code>[5]</code>.</p></dd></dl></section><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeEpsilonGreedy: $ \epsilon_n$-Greedy with a fixed exploration rate.</h4><p>This uses a fixed exploration rate $ \epsilon$.</p></header><dl><dt class="spec module-type" id="module-type-EpsilonGreedyParam"><a href="#module-type-EpsilonGreedyParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-EpsilonGreedyParam/index.html">EpsilonGreedyParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeEpsilonGreedy</code> .</p></dd></dl><dl><dt class="spec module" id="module-MakeEpsilonGreedy"><a href="#module-MakeEpsilonGreedy" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeEpsilonGreedy/index.html">MakeEpsilonGreedy</a> : <span class="keyword">functor</span> (<a href="MakeEpsilonGreedy/argument-1-P/index.html">P</a> : <a href="index.html#module-type-EpsilonGreedyParam">EpsilonGreedyParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeEpsilonGreedy/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditEstimates">banditEstimates</a></code></dt><dd><p>The Epsilon-Greedy Bandit with a fixed exploration rate.</p></dd></dl></section></section><section><header><h3 id="mdf"><a href="#mdf" class="anchor"></a>The Exp3 family of algorithms.</h3></header><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeExp3: EXP3 with a parametrizable rate.</h4><p>At round $ t$, the EXP3 algorithm<code>[1]</code> draws an arm from a probability distribution $ p$ and updates this distribution with a softmax operator:</p><p>$ p_{i,t+1} = \frac{\exp ( - \eta_t \widetilde{L_{i,t}})}{\sum{k=1}^{K}\text{exp}(-\eta_t \widetilde{L_{k,t}})} $</p><p>where $\widetilde{L_{i,t}}$ is the cumulative probability-normalized loss at time $ t$ of arm $i$, $\eta_t$ is the rate at time $t$.</p></header><dl><dt class="spec type" id="type-banditPolicy"><a href="#type-banditPolicy" class="anchor"></a><code><span class="keyword">type</span> banditPolicy</code><code> = </code><code>{</code><table class="record"><tr id="type-banditPolicy.t" class="anchored"><td class="def field"><a href="#type-banditPolicy.t" class="anchor"></a><code>t : int;</code></td><td class="doc"><p>The index of the step $t$.</p></td></tr><tr id="type-banditPolicy.a" class="anchored"><td class="def field"><a href="#type-banditPolicy.a" class="anchor"></a><code>a : int;</code></td><td class="doc"><p>The last action taken.</p></td></tr><tr id="type-banditPolicy.w" class="anchored"><td class="def field"><a href="#type-banditPolicy.w" class="anchor"></a><code>w : float list;</code></td><td class="doc"><p>The weights of the arm that define the policy.</p></td></tr></table><code>}</code></dt><dd><p>The internal state of an Exp3 bandit</p></dd></dl><dl><dt class="spec module" id="module-MakeExp3"><a href="#module-MakeExp3" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeExp3/index.html">MakeExp3</a> : <span class="keyword">functor</span> (<a href="MakeExp3/argument-1-P/index.html">P</a> : <a href="index.html#module-type-RateBanditParam">RateBanditParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeExp3/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditPolicy">banditPolicy</a></code></dt><dd><p>The Exp3 Bandit for adversarial regret minimization with a parametrizable learning rate.</p></dd></dl></section><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeDecayingExp3: EXP3 with the decaying rate from <code>[1]</code>.</h4><p>This variant uses the learning rate decay:</p><p>$$ \eta_t = \sqrt{\frac{ln K}{tK}} $$</p><p>and enjoys the pseudo-regret bound: $$ \bar{R_n} \leq 2 \sqrt{nK \ln K} $$</p></header><dl><dt class="spec module" id="module-MakeDecayingExp3"><a href="#module-MakeDecayingExp3" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeDecayingExp3/index.html">MakeDecayingExp3</a> : <span class="keyword">functor</span> (<a href="MakeDecayingExp3/argument-1-P/index.html">P</a> : <a href="index.html#module-type-KBanditParam">KBanditParam</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeDecayingExp3/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditPolicy">banditPolicy</a></code></dt><dd><p>The Exp3 Bandit for adversarial regret minimization with a decaying learning rate as per <code>[1]</code>.</p></dd></dl></section><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeFixedExp3: EXP3 with a fixed rate.</h4><p>This uses a fixed rate $\eta$.</p></header><dl><dt class="spec module-type" id="module-type-FixedExp3Param"><a href="#module-type-FixedExp3Param" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-FixedExp3Param/index.html">FixedExp3Param</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeFixedExp3</code> .</p></dd></dl><dl><dt class="spec module" id="module-MakeFixedExp3"><a href="#module-MakeFixedExp3" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeFixedExp3/index.html">MakeFixedExp3</a> : <span class="keyword">functor</span> (<a href="MakeFixedExp3/argument-1-P/index.html">P</a> : <a href="index.html#module-type-FixedExp3Param">FixedExp3Param</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeFixedExp3/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditPolicy">banditPolicy</a></code></dt><dd><p>The Exp3 Bandit for adversarial regret minimization with a decaying learning rate as per <code>[1]</code>.</p></dd></dl></section><section><header><h4 id="mdf"><a href="#mdf" class="anchor"></a>MakeHorizonExp3: EXP3 with a known horizon <code>[1]</code>.</h4><p>This variant optimizes for a known horizon $ n$ and uses the learning rate:</p><p>$$ \eta = \sqrt{\frac{2 ln K}{nK}} $$</p><p>It has the pseudo-regret bound:</p><p>$$ \bar{R_n} \leq \sqrt{2 nK \ln K} $$</p></header><dl><dt class="spec module-type" id="module-type-HorizonExp3Param"><a href="#module-type-HorizonExp3Param" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-HorizonExp3Param/index.html">HorizonExp3Param</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>Use to instanciate a <code>Bandit</code> from <code>MakeHorizonExp3</code> .</p></dd></dl><dl><dt class="spec module" id="module-MakeHorizonExp3"><a href="#module-MakeHorizonExp3" class="anchor"></a><code><span class="keyword">module</span> <a href="MakeHorizonExp3/index.html">MakeHorizonExp3</a> : <span class="keyword">functor</span> (<a href="MakeHorizonExp3/argument-1-P/index.html">P</a> : <a href="index.html#module-type-HorizonExp3Param">HorizonExp3Param</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-Bandit">Bandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="MakeHorizonExp3/index.html#type-bandit">bandit</a> = <a href="index.html#type-banditPolicy">banditPolicy</a></code></dt><dd><p>The Exp3 Bandit for adversarial regret minimization with a horizon-based learning rate as per <code>[1]</code>.</p></dd></dl></section></section><section><header><h3 id="mdf"><a href="#mdf" class="anchor"></a>More Functors: The doubling trick.</h3><p>Reward normalization in online stochastic and/or adversarial learning is a hard problem. While this is well studied in online learning <code>[2]</code><code>[3]</code><code>[4]</code>, there is no well studied procedure for bandits yet. The <code>WrapRange</code> Functors applies the heuristic solution known as the doubling trick.</p><p>The WrapRange functor wraps a bandit algorithm with the doubling trick. This heuristic allows to use a bandit algorithm without knowing the reward ranges. All rewards are linearly rescaled to a range (initially given by a RangeParam). When a value is observed above the range, the bandit algorithm is restarted and the range interval is doubled in that direction.</p><p>A convenience <code>WrapRange01</code> is provided for rewards that are initially thought to lie in $\left[0,1\right]$.</p></header><dl><dt class="spec module-type" id="module-type-RangeParam"><a href="#module-type-RangeParam" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-RangeParam/index.html">RangeParam</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>A Reward range.</p></dd></dl><dl><dt class="spec type" id="type-rangedAction"><a href="#type-rangedAction" class="anchor"></a><code><span class="keyword">type</span> rangedAction</code><code> = </code><table class="variant"><tr id="type-rangedAction.Reset" class="anchored"><td class="def constructor"><a href="#type-rangedAction.Reset" class="anchor"></a><code>| </code><code><span class="constructor">Reset</span> <span class="keyword">of</span> int</code></td></tr><tr id="type-rangedAction.Action" class="anchored"><td class="def constructor"><a href="#type-rangedAction.Action" class="anchor"></a><code>| </code><code><span class="constructor">Action</span> <span class="keyword">of</span> int</code></td></tr></table></dt><dd><p>A ranged action: Action a in normal course of action, Reset a in case * the bandit was just restarted.</p></dd></dl><dl><dt class="spec type" id="type-rangedBandit"><a href="#type-rangedBandit" class="anchor"></a><code><span class="keyword">type</span> 'b rangedBandit</code><code> = </code><code>{</code><table class="record"><tr id="type-rangedBandit.bandit" class="anchored"><td class="def field"><a href="#type-rangedBandit.bandit" class="anchor"></a><code>bandit : <span class="type-var">'b</span>;</code></td><td class="doc"><p>The original type of the bandit.</p></td></tr><tr id="type-rangedBandit.u" class="anchored"><td class="def field"><a href="#type-rangedBandit.u" class="anchor"></a><code>u : float;</code></td><td class="doc"><p>The upper reward bound.</p></td></tr><tr id="type-rangedBandit.l" class="anchored"><td class="def field"><a href="#type-rangedBandit.l" class="anchor"></a><code>l : float;</code></td><td class="doc"><p>The lower reward bound.</p></td></tr></table><code>}</code></dt><dd><p>The type of a bandit with a range.</p></dd></dl><dl><dt class="spec module-type" id="module-type-RangedBandit"><a href="#module-type-RangedBandit" class="anchor"></a><code><span class="keyword">module</span> <span class="keyword">type</span> <a href="module-type-RangedBandit/index.html">RangedBandit</a> = <span class="keyword">sig</span> ... <span class="keyword">end</span></code></dt><dd><p>The type of a bandit with reward scaling.</p></dd></dl><dl><dt class="spec module" id="module-WrapRange"><a href="#module-WrapRange" class="anchor"></a><code><span class="keyword">module</span> <a href="WrapRange/index.html">WrapRange</a> : <span class="keyword">functor</span> (<a href="WrapRange/argument-1-R/index.html">R</a> : <a href="index.html#module-type-RangeParam">RangeParam</a>) <span>&#45;&gt;</span> <span class="keyword">functor</span> (<a href="WrapRange/argument-2-B/index.html">B</a> : <a href="index.html#module-type-Bandit">Bandit</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-RangedBandit">RangedBandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="WrapRange/index.html#type-bandit">bandit</a> = <a href="WrapRange/argument-2-B/index.html#type-bandit">B.bandit</a></code></dt><dd><p>The WrapRange functor wraps a bandit algorithm with the doubling trick. This heuristic allows to use a bandit algorithm without knowing the reward ranges. All rewards are linearly rescaled to a range (initially given by a RangeParam). When a value is observed above the range, the bandit algorithm is restarted and the range interval is doubled in that direction.</p></dd></dl><dl><dt class="spec module" id="module-WrapRange01"><a href="#module-WrapRange01" class="anchor"></a><code><span class="keyword">module</span> <a href="WrapRange01/index.html">WrapRange01</a> : <span class="keyword">functor</span> (<a href="WrapRange01/argument-1-B/index.html">B</a> : <a href="index.html#module-type-Bandit">Bandit</a>) <span>&#45;&gt;</span> <a href="index.html#module-type-RangedBandit">RangedBandit</a> <span class="keyword">with</span> <span class="keyword">type</span> <a href="WrapRange01/index.html#type-bandit">bandit</a> = <a href="WrapRange01/argument-1-B/index.html#type-bandit">B.bandit</a></code></dt><dd><p>The WrapRange01 functor is a convenience aliasing of WrapRange with an initial &quot;standard&quot; range of $ \left[ 0,1 \right] $.</p></dd></dl></section><section><header><h2 id="ex"><a href="#ex" class="anchor"></a>Examples</h2><p>see test/test.ml for an example of bandit use.</p></header></section><section><header><h2 id="refs"><a href="#refs" class="anchor"></a>References</h2><p><code>[1]</code> <a href="http://arxiv.org/abs/1204.5721">Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</a>, Sebastien Bubeck and Nicolo Cesa-Bianchi.</p><p><code>[2]</code> <a href="http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient methods for Online Learning and Stochastic Optimization</a>, John Duchi , Elad Hazan and Yoram Singer.</p><p><code>[3]</code> <a href="http://arxiv.org/abs/1305.6646">Normalized Online Learning</a>, Stephane Ross, Paul Mineiro, John Langford</p><p><code>[4]</code> <a href="http://arxiv.org/abs/1601.01974">Scale-Free Online Learning</a>, Francesco Orabona, Dávid Pál</p><p><code>[5]</code> <a href="http://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf">Finite-time Analysis of the Multiarmed Bandit Problem</a>, Peter Auer, Nicolo Cesa-Bianchi, Paul Fischer</p></header></section></div></body></html>